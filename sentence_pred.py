# -*- coding: utf-8 -*-
"""English to Spanish.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqJb4A4rfo6WfeAeHHfMflY-j5l89Nrh

Canary Large Language Model


```
# This is formatted as code
```

# Word Predictional Model (Official)
"""

!pip install transformers datasets
!pip install fsspec==2024.9.0

import pandas as pd
import random
from nltk import ngrams  # Import ngrams for sequence generation
import numpy as np
import torch
from sklearn.model_selection import train_test_split
# from transformers import BertTokenizer
from transformers import BertTokenizer, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader

# Step 1: Load Dataset
from google.colab import files
uploaded = files.upload()  # Upload your CSV file
df = pd.read_csv(list(uploaded.keys())[0])
df = df[['english', 'spanish']]  # Ensure correct column names

# Step 2: Create Test Data
def create_test_data(df, n, language='english'):
    """
    Creates test data for next-word prediction for the specified language.
    """
    inputs = []
    targets = []
    sentences = df[language].tolist()  # Get sentences for the specified language

    for sentence in sentences:
        words = sentence.split()  # Split sentence into words
        for sequence in ngrams(words, n):  # Generate n-grams
            inputs.append(" ".join(sequence[:-1]))  # Input is sequence except last word
            targets.append(sequence[-1])  # Target is the last word
    return inputs, targets

# Creating Test Data for both English and Spanish Data
english_inputs, english_targets = create_test_data(df, 3, language='english')
spanish_inputs, spanish_targets = create_test_data(df, 3, language='spanish')

# print(f"English Test Data: {(english_inputs)}")
# print(f"Spanish Test Data: {(spanish_inputs)}")

# Split into train and test (80% train, 20% test)
# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Further split train into train and validation (80% train, 20% validation)
# train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

# Splitting english_inputs and english_targets into train and test (80% train, 20% test)
english_train_inputs, english_test_inputs, english_train_targets, english_test_targets = train_test_split(
    english_inputs, english_targets, test_size=0.2, random_state=42
)
# Further split train into train and validation (80% train, 20% validation)
english_train_inputs, english_val_inputs, english_train_targets, english_val_targets = train_test_split(
    english_train_inputs, english_train_targets, test_size=0.2, random_state=42
)

# For Spanish Data
spanish_train_inputs, spanish_test_inputs, spanish_train_targets, spanish_test_targets = train_test_split(
    spanish_inputs, spanish_targets, test_size=0.2, random_state=42
)
spanish_train_inputs, spanish_val_inputs, spanish_train_targets, spanish_val_targets = train_test_split(
    spanish_train_inputs, spanish_train_targets, test_size=0.2, random_state=42
)

# Choose and Initialize Model
# Use BertForMaskedLM for next-word prediction
model = BertForMaskedLM.from_pretrained("bert-base-multilingual-cased")
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Prepare Training Data
class TextDataset(Dataset):
    def __init__(self, inputs, targets, tokenizer, max_len):
        self.inputs = inputs
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        text = self.inputs[idx]  # Get input text
        target = self.targets[idx]  # Get target text (NEW)

         # Combine input text and target with [MASK] token
        masked_text = text + " [MASK]"

        # Tokenize the masked text
        inputs = self.tokenizer(
            masked_text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors="pt",
            add_special_tokens=True
        )

        # Tokenize the target and get its token ID
        target_token_id = self.tokenizer.convert_tokens_to_ids(target)

        # Create labels tensor. -100 is used for tokens that should be ignored
        labels = inputs['input_ids'].clone()
        labels[labels != self.tokenizer.mask_token_id] = -100  # Ignore all tokens except [MASK]

        # Set the label for the [MASK] token to the target token ID
        mask_index = torch.where(labels == self.tokenizer.mask_token_id)[1]
        labels[:, mask_index] = target_token_id

        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': labels.squeeze(0) # Labels are now a sequence with -100 for ignored tokens
        }

max_length = 128

# Create datasets and dataloaders for English and Spanish
english_train_dataset = TextDataset(english_train_inputs, english_train_targets, tokenizer, max_len=max_length)
english_train_dataloader = DataLoader(english_train_dataset, batch_size=16, shuffle=True)

spanish_train_dataset = TextDataset(spanish_train_inputs, spanish_train_targets, tokenizer, max_len=max_length)
spanish_train_dataloader = DataLoader(spanish_train_dataset, batch_size=16, shuffle=True)

# 3. Set up Optimizer and Scheduler
# optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8) #Depreciated version
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5) #lr=5e-5

# Calculate total steps for both English and Spanish data
# total_steps = len(english_train_dataloader) * epochs + len(spanish_train_dataloader) * epochs
epochs = 6  # Adjust as needed
total_steps = len(english_train_dataloader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# # 4. Training Loop
# for epoch in range(epochs):
#     model.train()
#     total_loss = 0

#     # Train on English data
#     for batch in english_train_dataloader:
#         optimizer.zero_grad()
#         input_ids = batch['input_ids'].to(device)
#         attention_mask = batch['attention_mask'].to(device)
#         labels = batch['labels'].to(device)

#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
#         loss = outputs.loss
#         total_loss += loss.item()
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: Clip gradients
#         optimizer.step()
#         scheduler.step()  # Update scheduler after each batch

#     for batch in spanish_train_dataloader:
#         optimizer.zero_grad()
#         input_ids = batch['input_ids'].to(device)
#         attention_mask = batch['attention_mask'].to(device)
#         labels = batch['labels'].to(device)

#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
#         loss = outputs.loss
#         total_loss += loss.item()
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: Clip gradients
#         optimizer.step()
#         scheduler.step()  # Update scheduler after each batch

#     print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (len(english_train_dataloader) + len(spanish_train_dataloader))}")
#     # print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (len(english_train_dataloader))}")

# 4. Training Loop
for epoch in range(epochs):
    model.train()
    total_loss = 0

    # Train on English data
    for batch in english_train_dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: Clip gradients
        optimizer.step()
        scheduler.step()  # Update scheduler after each batch

    #Train on Spanish data
    for batch in spanish_train_dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: Clip gradients
        optimizer.step()
        scheduler.step()  # Update scheduler after each batch

    # print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (len(english_train_dataloader) + len(spanish_train_dataloader))}")
    print(f"English Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (len(english_train_dataloader))}")
    print(f"Spanish Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (len(spanish_train_dataloader))}")

from sklearn.metrics import accuracy_score

def evaluate_model(model, dataloader, device):
    model.eval()  # Set the model to evaluation mode
    all_predictions = []
    all_targets = []

    with torch.no_grad():  # Disable gradient calculations during evaluation
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)  # Get the true labels

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            # Get predicted token IDs
            predicted_token_ids = torch.argmax(outputs.logits, dim=-1)

            # Get the predictions for the masked token (index is where label is not -100)
            mask_index = torch.where(labels != -100)[1]
            batch_predictions = predicted_token_ids[torch.arange(predicted_token_ids.shape[0]), mask_index]
            batch_targets = labels[torch.arange(labels.shape[0]), mask_index]  # Get the true target tokens

            all_predictions.extend(batch_predictions.cpu().numpy())
            all_targets.extend(batch_targets.cpu().numpy())

    # Calculate accuracy
    accuracy = accuracy_score(all_targets, all_predictions)

    return accuracy

# Create the test dataset and dataloader
english_test_dataset = TextDataset(english_test_inputs, english_test_targets, tokenizer, max_len=max_length)
english_test_dataloader = DataLoader(english_test_dataset, batch_size=16)

spanish_test_dataset = TextDataset(spanish_test_inputs, spanish_test_targets, tokenizer, max_len=max_length)
spanish_test_dataloader = DataLoader(spanish_test_dataset, batch_size=16)

# Evaluate the model on English test data
accuracy1 = evaluate_model(model, english_test_dataloader, device)
accuracy2 = evaluate_model(model, spanish_test_dataloader, device)
print(f"English Test Accuracy: {accuracy1}")
print(f"Spanish Test Accuracy: {accuracy2}")

# 5. Save the Model
model.save_pretrained("language_model_clm")
tokenizer.save_pretrained("language_model_clm")